{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Spacy\n","\n","In this part, we will see how we can use spacy to perform sentiment analysis.\n","\n","I have tried various ways to use spacy and compared the various accuracy values because there are not many hyperparameters to tune here."],"metadata":{"id":"HwRvJk_y0Mc9"}},{"cell_type":"markdown","source":["First, we will see how we can use spacy for visualizations"],"metadata":{"id":"V1TxSQAh0lA_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4O4mdw0H0Ltp"},"outputs":[],"source":["#Import Modules\n","import pandas as pd\n","import numpy as np\n","import spacy\n","from spacy import displacy\n","from spacy.util import minibatch, compounding\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"BkrHKKNnBAJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path=\"/content/drive/MyDrive/519 Project/Code for Submission/Copy of Reviews(1).csv\""],"metadata":{"id":"DtYPlPlhEcST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If any EOF error occurs, you can include the commented part. For some reason, I noticed EOF errors when the drive is not mounted. Mounting the drive and using that path helps."],"metadata":{"id":"3C3mc29VFFmF"}},{"cell_type":"code","source":["food_reviews_df=pd.read_csv(path)#,quoting=3, error_bad_lines=False)\n","food_reviews_df.shape"],"metadata":{"id":"Yuwr49fJ0j6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["food_reviews_df = food_reviews_df[['Text','Score']].dropna()"],"metadata":{"id":"v-_bsxn41Slv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_tok = spacy.load('en_core_web_sm')\n","sample_review=food_reviews_df.Text[54]\n","sample_review"],"metadata":{"id":"46sOoq2s1UeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parsed_review = spacy_tok(sample_review)\n","parsed_review"],"metadata":{"id":"L6ZeOQkH1WIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/tylerneylon/explacy/master/explacy.py"],"metadata":{"id":"XEF322n31YGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import explacy\n","explacy.print_parse_info(spacy_tok, 'The salad was surprisingly tasty.')"],"metadata":{"id":"UeU_RghN1Z26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explacy.print_parse_info(spacy_tok,food_reviews_df.Text[0])"],"metadata":{"id":"9yn4tiA51cCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_text = pd.DataFrame()\n","\n","for i, token in enumerate(parsed_review):\n","    tokenized_text.loc[i, 'text'] = token.text\n","    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n","    tokenized_text.loc[i, 'pos'] = token.pos_\n","    tokenized_text.loc[i, 'tag'] = token.tag_\n","    tokenized_text.loc[i, 'dep'] = token.dep_\n","    tokenized_text.loc[i, 'shape'] = token.shape_\n","    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n","    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n","    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n","\n","tokenized_text[:20]"],"metadata":{"id":"f1-LwFzG1hGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy.displacy.render(parsed_review, style='ent', jupyter=True)"],"metadata":{"id":"ozrR3KlN1jdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence_spans = list(parsed_review.sents)\n","sentence_spans"],"metadata":{"id":"IB0XK9Fo1la4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["displacy.render(parsed_review, style='dep', jupyter=True,options={'distance': 140})"],"metadata":{"id":"5ncY-QPJ1nbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["options = {'compact': True, 'bg': 'violet','distance': 140,\n","           'color': 'white', 'font': 'Trebuchet MS'}\n","displacy.render(parsed_review, jupyter=True, style='dep', options=options)"],"metadata":{"id":"rs8sDh8U1pG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noun_chunks_df = pd.DataFrame()\n","\n","for i, chunk in enumerate(parsed_review.noun_chunks):\n","    noun_chunks_df.loc[i, 'text'] = chunk.text\n","    noun_chunks_df.loc[i, 'root'] = chunk.root,\n","    noun_chunks_df.loc[i, 'root.text'] = chunk.root.text,\n","    noun_chunks_df.loc[i, 'root.dep_'] = chunk.root.dep_\n","    noun_chunks_df.loc[i, 'root.head.text'] = chunk.root.head.text\n","\n","noun_chunks_df[:20]"],"metadata":{"id":"k6kR_-Ba1rO1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will look at creating various types of models for spacy."],"metadata":{"id":"KBJXCIpb111L"}},{"cell_type":"code","source":["from spacy.lang.en.stop_words import STOP_WORDS\n","df=pd.read_csv(\"/content/drive/MyDrive/Reviews(1).csv\")#,quoting=3, error_bad_lines=False)\n","df_5=df[df[\"Score\"]==5][:200]\n","df_4=df[df[\"Score\"]==4][:200]\n","df_3=df[df[\"Score\"]==3][:200]\n","df_2=df[df[\"Score\"]==2][:200]\n","df_1=df[df[\"Score\"]==1][:200]\n","df=pd.concat([df_5, df_4,df_3,df_2,df_1], axis=0)\n","df = df.sample(frac=1)"],"metadata":{"id":"zPwtl5jx11Hn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg\n"],"metadata":{"id":"6kt6EIFr2cng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I have written the pre-processing function for the texts"],"metadata":{"id":"DAzBFhuSzZio"}},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_lg\")\n","# define function to preprocess text\n","def preprocess_text(text):\n","    # convert text to lowercase\n","    text = text.lower()\n","    # create doc object using spaCy model\n","    doc = nlp(text)\n","    # return doc\n","    # lemmatize and remove stop words\n","    tokens = [token.lemma_ for token in doc if not token.is_stop]\n","\n","    # join tokens back into string and return\n","    return ' '.join(tokens)\n","\n","# apply preprocessing to text column in dataframe\n","df['clean_text'] = df['Text'].apply(preprocess_text)\n","\n","\n","# print first 5 rows of dataframe with cleaned text column\n","df[[\"Text\",\"clean_text\"]].head()"],"metadata":{"id":"i23T6VxG1-v_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ML Packages\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer \n","from sklearn.metrics import accuracy_score \n","from sklearn.model_selection import train_test_split \n","from sklearn.base import TransformerMixin \n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline \n","import numpy as np"],"metadata":{"id":"_bjLRJSW2B7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_text(text):\n","  return preprocess_text(text)\n","\n","# define custom transformers class\n","class Transformers(TransformerMixin):\n","    def transform(self, X, **transform_params):\n","        return [clean_text(text) for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","    def get_params(self, deep=True):\n","        return {}"],"metadata":{"id":"3L86fuL32F1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split dataset\n","X = df['Text'].apply(clean_text)\n","y = df['Score']\n","y=y.apply(lambda x: 1 if x>=4 else 0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"],"metadata":{"id":"u69N5ZO02Mre"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, I have created a spacy tokenizer function to use as features. We extract tokens from the doc object obtained by using the nlp object on the text."],"metadata":{"id":"ZqjzZpc6fqCj"}},{"cell_type":"code","source":["# define vectorizer and classifier\n","# vectorizer = CountVectorizer(tokenizer=lambda text: nlp(text), ngram_range=(1,1))\n","def spacy_tokenizer(text):\n","    return [token.text for token in nlp(text)]\n","\n","vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n","\n","classifier = LinearSVC()\n","\n","# create pipeline\n","pipe = Pipeline([\n","    ('cleaner', Transformers()),\n","    ('vectorizer', vectorizer),\n","    ('classifier', classifier)\n","])\n","\n","# train model\n","pipe.fit(X_train, y_train)\n","\n","# save the vectorizer\n","vectorizer = pipe.named_steps['vectorizer']\n","\n","# evaluate model\n","print(\"Accuracy:\", pipe.score(X_test, y_test))\n","\n","\n","#Accuracy=0.676 (Before)\n","#Accuracy=0.736 (After)"],"metadata":{"id":"hs9wgsqs2Ovq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next Part"],"metadata":{"id":"sYxzasMa2Wre"}},{"cell_type":"code","source":["import pandas as pd\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import TransformerMixin\n","from sklearn.metrics import accuracy_score\n","\n","df = pd.read_csv(path)#,quoting=3, error_bad_lines=False)\n","df_5=df[df[\"Score\"]==5][:200]\n","df_4=df[df[\"Score\"]==4][:200]\n","df_3=df[df[\"Score\"]==3][:200]\n","df_2=df[df[\"Score\"]==2][:200]\n","df_1=df[df[\"Score\"]==1][:200]\n","df=pd.concat([df_5, df_4,df_3,df_2,df_1], axis=0)\n","\n","# print(df.head(5))\n","\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","def preprocess_text(text):\n","    doc = nlp(text.lower())\n","    tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    return ' '.join(tokens)\n","\n","df['clean_text'] = df['Text'].apply(preprocess_text)\n","\n","\n","\n","\n","\n"],"metadata":{"id":"g7f7FqQe2WH9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, I tried to create a spacy vectorizer class which I use to create a feature space of text vectors obtained from the nlp object on the text."],"metadata":{"id":"knqDNRjkgAMf"}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator\n","from spacy.pipeline import Sentencizer\n","\n","#Create a custom SpacyVectorizer transformer class that transforms the input text into spaCy's Doc vectors:\n","class SpacyVectorizer(TransformerMixin):\n","    def transform(self, X, **transform_params):\n","        return [nlp(text).vector for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","    def get_params(self, deep=True):\n","        return {}\n","\n","\n","\n","\n","X = df['clean_text']\n","y = df['Score'].apply(lambda x:1 if x>=4 else 0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"],"metadata":{"id":"-8ft8CHl2hrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","vectorizer = SpacyVectorizer()\n","classifier = LinearSVC()\n","\n","pipe = Pipeline([\n","    ('vectorizer', vectorizer),\n","    ('classifier', classifier)\n","])\n","\n","pipe.fit(X_train, y_train)\n","\n","print(\"Accuracy:\", pipe.score(X_test, y_test))\n","\n","\n","#Accuracy=0.328(Before)\n","#0.704 (After)"],"metadata":{"id":"WFA5PxoW2inW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, I created a Spacy embeddings class to try to create a feature space of embeddings"],"metadata":{"id":"zADO4d6V2sNy"}},{"cell_type":"code","source":["import pandas as pd\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import TransformerMixin\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","df = pd.read_csv(path)#,quoting=3, error_bad_lines=False)\n","df_5=df[df[\"Score\"]==5][:200]\n","df_4=df[df[\"Score\"]==4][:200]\n","df_3=df[df[\"Score\"]==3][:200]\n","df_2=df[df[\"Score\"]==2][:200]\n","df_1=df[df[\"Score\"]==1][:200]\n","df=pd.concat([df_5, df_4,df_3,df_2,df_1], axis=0)\n","\n","# print(df.head(5))\n","\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","def preprocess_text(text):\n","    doc = nlp(text)\n","    tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n","    return \" \".join([token.lemma_.lower() for token in tokens])\n","\n","df[\"Clean_Text\"] = df[\"Text\"].apply(preprocess_text)\n","df[\"Score\"]=df[\"Score\"].apply(lambda x:1 if x>=4 else 0)\n","\n","\n","# Define a custom transformer that uses spaCy to tokenize the text and extract word embeddings\n","class SpacyEmbeddings(TransformerMixin):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.dim = len(nlp(\"apple\").vector)\n","    \n","    def transform(self, X, y=None):\n","        return np.array([self.nlp(text).vector for text in X])\n","    \n","    def fit(self, X, y=None):\n","        return self\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df[\"Clean_Text\"], df[\"Score\"], test_size=0.2, random_state=42)\n","\n","# Define the pipeline\n","pipeline = Pipeline([\n","    (\"embeddings\", SpacyEmbeddings(nlp)),\n","    (\"classifier\", LinearSVC())\n","])\n","\n","# Fit the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = pipeline.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy: {:.3f}\".format(accuracy))\n","\n","\n","#Accuracy=0.330(Before)\n","#0.685(After)"],"metadata":{"id":"HCOXF_GQ2ofD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, I am trying to use n-grams, more specifically unigrams and bigrams in my feature space"],"metadata":{"id":"itefF4wT2yyc"}},{"cell_type":"code","source":["import spacy\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report\n","\n","# Load the language model\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[\"Clean_Text\"], df[\"Score\"], test_size=0.2, random_state=42)\n","\n","# Define a custom tokenizer using spacy\n","def spacy_tokenizer(text):\n","    doc = nlp(text)\n","    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n","\n","# Define the pipeline\n","pipeline = Pipeline([\n","    ('vect', CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,2))),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', LinearSVC())\n","])\n","\n","# Fit the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = pipeline.predict(X_test)\n","\n","# Evaluate the model\n","print(classification_report(y_test, y_pred))\n","\n","\n","\n","#Accuracy=0.46(Before)\n","#0.78 (After)"],"metadata":{"id":"Cr6Yxk-B20Mz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","n-grams attempt 2:\n","\n","Another attempt at n-grams\n","\n"],"metadata":{"id":"EyHe7N3Rnawj"}},{"cell_type":"code","source":["import pandas as pd\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Load the data and select a subset of reviews for each score\n","df = pd.read_csv(path)#,quoting=3, error_bad_lines=False)\n","df_5 = df[df[\"Score\"]==5][:4000]\n","df_4 = df[df[\"Score\"]==4][:4000]\n","df_3 = df[df[\"Score\"]==3][:4000]\n","df_2 = df[df[\"Score\"]==2][:4000]\n","df_1 = df[df[\"Score\"]==1][:4000]\n","\n","# Concatenate the dataframes\n","df = pd.concat([df_5, df_4, df_3, df_2, df_1], axis=0)\n","\n","# Define a function to preprocess the text using Spacy and generate n-grams\n","nlp = spacy.load('en_core_web_lg')\n","def preprocess(text):\n","    doc = nlp(text)\n","    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n","    n = 3\n","    ngrams = []\n","    for i in range(len(tokens) - n + 1):\n","        ngrams.append(' '.join(tokens[i:i+n]))\n","    return ' '.join(ngrams)\n","\n","# Preprocess the text\n","df['Text'] = df['Text'].apply(preprocess)\n","df[\"Score\"]=df[\"Score\"].apply(lambda x:1 if x>=4 else 0)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Score'], test_size=0.2, random_state=42)\n","\n","# Define the pipeline\n","pipeline = Pipeline([\n","    ('vectorizer', TfidfVectorizer()),\n","    ('classifier', MultinomialNB())\n","])#LinearSVC()\n","\n","# Define the hyperparameters to tune\n","parameters = {\n","    'vectorizer__max_features': [1000, 5000, 10000],\n","    'vectorizer__ngram_range': [(1,1), (1,2), (2,2)],\n","    'classifier__alpha': [1, 0.1, 0.01, 0.001]\n","}\n","\n","# Perform the grid search\n","grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = grid_search.predict(X_test)\n","\n","# Print the classification report and confusion matrix\n","print(classification_report(y_test, y_pred))\n","print(confusion_matrix(y_test, y_pred))\n","#Accuracy=0.44(Before)\n","#Accuracy=0.80 (After)"],"metadata":{"id":"f-H4GOat26vn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trying to use TfidVectorizer in a different way"],"metadata":{"id":"6vd850WQ2_ab"}},{"cell_type":"code","source":["# ML Packages\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.base import TransformerMixin\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline\n","\n","vectorizer = TfidfVectorizer(tokenizer=lambda text: nlp(text), ngram_range=(1, 1))\n","classifier = LinearSVC()\n","\n","pipe = Pipeline([\n","    ('cleaner', Transformers()),\n","    ('vectorizer', vectorizer),\n","    ('classifier', classifier)\n","])\n","\n","pipe.fit(X_train, y_train)\n","print(\"Accuracy:\", pipe.score(X_test, y_test))\n","\n","\n","\n","#Accuracy=0.604"],"metadata":{"id":"GJ_vBP0M3CYC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simple Spacy Implementation\n","\n","I have shown a very simple and straightforward spacy implementation that is easy to understand, which yielded the best accuracy.\n","\n","The above was true at first before I tweaked my other attempt at n-grams a bit. I made similar changes to all my previous models and I have marked the \"before\" and \"after\" accuracies accordingly. \n","\n","Now, my best performing model is the one where I used n-grams with an accuracy of 0.79"],"metadata":{"id":"Yd_RB02VRLTr"}},{"cell_type":"code","source":["import pandas as pd\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","df=pd.read_csv(path)#,quoting=3, error_bad_lines=False)\n","df_5=df[df[\"Score\"]==5][:200]\n","df_4=df[df[\"Score\"]==4][:200]\n","df_3=df[df[\"Score\"]==3][:200]\n","df_2=df[df[\"Score\"]==2][:200]\n","df_1=df[df[\"Score\"]==1][:200]\n","df=pd.concat([df_5, df_4,df_3,df_2,df_1], axis=0)"],"metadata":{"id":"1K1ZQecxRU3j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load the large model\n","nlp = spacy.load(\"en_core_web_lg\")"],"metadata":{"id":"UQ42etmPRb2o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocessing using the nlp object we created"],"metadata":{"id":"mwkmDhwySEP2"}},{"cell_type":"code","source":["# define function to preprocess text\n","def preprocess_text(text):\n","    # convert text to lowercase\n","    text = text.lower()\n","    # create doc object using spaCy model\n","    doc = nlp(text)\n","    # return doc\n","    # lemmatize and remove stop words\n","    tokens = [token.lemma_ for token in doc if not token.is_stop]\n","\n","    # join tokens back into string and return\n","    return ' '.join(tokens)\n","\n","# apply preprocessing to text column in dataframe\n","df['clean_text'] = df['Text'].apply(preprocess_text)\n","\n","# print first 5 rows of dataframe with cleaned text column\n","df[[\"Text\",\"clean_text\"]].head()"],"metadata":{"id":"r3j8HhBTRde4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We obtain the vectors using the nlp object as we split the dataset"],"metadata":{"id":"4cpUZ_a6SL_O"}},{"cell_type":"code","source":["# split dataset \n","X = df['clean_text']\n","y = df['Score']\n","document=nlp.pipe(X)\n","text_vector=np.array([text.vector for text in document])\n","X=text_vector\n","y = y.apply(lambda x:1 if x>=4 else 0)\n","# y = y.apply(lambda x: 0 if x < 3 else (1 if x > 3 else x))\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"],"metadata":{"id":"16hIgY4pRd5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We import and run the logistic regression model"],"metadata":{"id":"gyCmYVtvSTz9"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","model=LogisticRegression(max_iter=1000,C=10)\n","X_train.shape,y_train.shape\n","model.fit(X_train,y_train)"],"metadata":{"id":"Zg96bljjRhrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred=model.predict(X_test)\n","print(model.score(X_test,y_test))\n","\n","#Accuracy=0.76 "],"metadata":{"id":"Lacw0LcyRiOW"},"execution_count":null,"outputs":[]}]}